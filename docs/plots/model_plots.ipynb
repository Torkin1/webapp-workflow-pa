{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "\n",
    "def derived_metrics_calc(num_denum_df, obj, num_denum_dict):\n",
    "    num_denum_df.to_csv('test.csv', index=False)\n",
    "    num_denum_df.drop(columns=['stat'], inplace=True)\n",
    "    metric_list = []\n",
    "    for key in num_denum_dict.keys():\n",
    "        value = num_denum_dict[key][0]\n",
    "        metric = num_denum_dict[key][1]\n",
    "\n",
    "        num_df = num_denum_df[num_denum_df['metric'] == key]\n",
    "        denum_df = num_denum_df[num_denum_df['metric'] == value]\n",
    "\n",
    "        num_df = num_df.rename(columns={'value': key})\n",
    "        denum_df = denum_df.rename(columns={'value': value})\n",
    "\n",
    "        num_df = num_df.drop(columns=['metric'])\n",
    "        denum_df = denum_df.drop(columns=['metric'])\n",
    "\n",
    "        metric_df = pd.merge(num_df, denum_df, on=['lambda', 'iteration'], how='inner', validate='many_to_one')\n",
    "        metric_df.rename(columns={'node_x': 'node'}, inplace=True)\n",
    "        metric_df.drop(columns=['node_y'], inplace=True)\n",
    "        metric_df[metric] = metric_df[key] / metric_df[value]\n",
    "\n",
    "        metric_df = metric_df[['iteration','lambda','node',key,value,metric]]\n",
    "        metric_df.to_csv(os.path.join('output',f'{obj}_rho.csv'), index=False)\n",
    "        metric_df = metric_df.rename(columns={metric: 'value'})\n",
    "        metric_df['metric'] = metric\n",
    "        metric_df.to_csv('test1.csv', index=False)\n",
    "        curr_list = []\n",
    "        for node in metric_df['node'].unique():\n",
    "            node_df = metric_df[metric_df['node'] == node]\n",
    "            curr_list.append(node_df)\n",
    "        metric_list.append(curr_list)\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "\n",
    "All csv files containing run statistics are read and a single dataframe is created with the concatenated csvs by adding a 'lambda' column with the arrival rate values. Three other columns are extracted from the 'statistic' column: 'node' containing the reference node of the statistic or system if it is global, 'metric' indicating the measured metric and finally 'stat' specifying whether the measured value is mean, minimum, maximum or standard deviation.\n",
    "\n",
    "Finally, a list of dataframes is created one for each metric and node taking only the mean as the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(\".\", \"output\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Function to remove outliers based on Z-score\n",
    "def remove_outliers_zscore(df, column, threshold=3):\n",
    "    z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "    return df[z_scores <= threshold]\n",
    "\n",
    "def preprocessing(path, obj):\n",
    "    filenames = os.listdir(path)\n",
    "    stats_df_list = []\n",
    "    for i in filenames:\n",
    "        lambda_val = i.split('=')[1].split('_')[0]\n",
    "        stats_path = os.path.join(path, i)\n",
    "        stats_df = pd.read_csv(stats_path)\n",
    "        stats_df['lambda'] = lambda_val\n",
    "        stats_df_list.append(stats_df)\n",
    "    stats_df = pd.concat(stats_df_list)\n",
    "\n",
    "    stats_df['node'] = stats_df['statistic'].apply(lambda x: x.split('-')[0])\n",
    "    stats_df['metric'] = stats_df['statistic'].apply(lambda x: x.split('-')[1])\n",
    "    stats_df['stat'] = stats_df['statistic'].apply(lambda x: x.split('-')[2])\n",
    "    stats_df.drop(columns=['statistic'], inplace=True)\n",
    "\n",
    "    stats_df = stats_df[stats_df['stat'].isin(['avg', 'val'])]\n",
    "    \n",
    "    # List of one DataFrame for each metric selecting only the average as statistic\n",
    "    metrics = stats_df['metric'].unique()\n",
    "    metrics_avg_df_list = []\n",
    "    metrics = metrics[metrics != 'interarrival']\n",
    "    metrics = metrics[metrics != 'service']\n",
    "    metrics = metrics[metrics != 'simtime']\n",
    "    metrics = metrics[metrics != 'completions']\n",
    "    \n",
    "    for i in metrics:\n",
    "        metrics_avg_df = stats_df[stats_df['metric'] == i]\n",
    "        \n",
    "        # Remove outliers for the 'value' column (or other numerical columns)\n",
    "        if 'value' in metrics_avg_df.columns:\n",
    "            metrics_avg_df = remove_outliers_zscore(metrics_avg_df, 'value')\n",
    "        \n",
    "        metrics_avg_df_list.append(metrics_avg_df)\n",
    "\n",
    "    metrics_nodes_avg_df_list = []\n",
    "    for i in metrics_avg_df_list:\n",
    "        nodes = i['node'].unique()\n",
    "        node_avg_df_list = []\n",
    "        for j in nodes:\n",
    "            node_avg_df = i[i['node'] == j]\n",
    "            node_avg_df_list.append(node_avg_df)\n",
    "        metrics_nodes_avg_df_list.append(node_avg_df_list)\n",
    "    metric_dict = {'service':['simtime', 'utilization'], 'completions':['simtime', 'throughput']}\n",
    "    derived_metrics = derived_metrics_calc(stats_df, obj, metric_dict)\n",
    "    for i in derived_metrics:\n",
    "        metrics_nodes_avg_df_list.append(i)\n",
    "        \n",
    "    return metrics_nodes_avg_df_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot funcion\n",
    "Boxplots of the average of population number, throughput and response time with arrival rate in a range 0.5, 1.2 job/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(metrics_nodes_avg_df_list):    \n",
    "    for metric in metrics_nodes_avg_df_list:\n",
    "        figure, axis = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        for i, boxplot_df in enumerate(metric):\n",
    "            boxplot_df['value'].astype(float)\n",
    "            boxplot_df['lambda'].astype(float)\n",
    "            boxplot_df = boxplot_df.sort_values(by='lambda')\n",
    "\n",
    "            curr_metric = boxplot_df['metric'].unique()[0]\n",
    "            curr_node = boxplot_df['node'].unique()[0]\n",
    "            x = i // 2\n",
    "            y = i % 2\n",
    "\n",
    "            boxplot_df.boxplot(column='value', by='lambda', ax = axis[x, y], showfliers=False)\n",
    "            plt.suptitle('')\n",
    "            axis[x, y].set_title(f'Boxplot of {curr_metric} of the {curr_node} node')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval calculation\n",
    "Confidence intervals are calculated on the batches of runs made as the lambda parameter changes.\n",
    "\n",
    "The following formula is used to calculate the interval:\n",
    "\n",
    "$\n",
    "CI = \\bar{x} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}$ is the sample mean.\n",
    "- $z$ is the critical value of the standard normal distribution corresponding to the chosen confidence level (e.g., 1.96 for 95%).\n",
    "- $\\sigma$ is the population standard deviation.\n",
    "- $n$ is the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(metrics_nodes_avg_df_list, critical_value, obj):\n",
    "    ci_df = []\n",
    "    for metric in metrics_nodes_avg_df_list:\n",
    "        metric_list = []\n",
    "        for node_df in metric:\n",
    "            curr_metric = node_df['metric'].unique()[0]\n",
    "            curr_node = node_df['node'].unique()[0]\n",
    "            \n",
    "            df = node_df.groupby([\"lambda\"])['value'].describe()[[\"count\", \"mean\", \"std\"]].reset_index()\n",
    "            df[\"lower_ci\"] = df[\"mean\"] - critical_value*(df[\"std\"]/np.sqrt(df[\"count\"]))\n",
    "            df[\"upper_ci\"] = df[\"mean\"] + critical_value*(df[\"std\"]/np.sqrt(df[\"count\"]))\n",
    "\n",
    "            df.to_csv(os.path.join(\"output\", f\"{obj}_conf_int_{curr_node}_{curr_metric}.csv\"), index=False)\n",
    "            df['metric'] = curr_metric\n",
    "            df['node'] = curr_node\n",
    "            metric_list.append(df)\n",
    "        ci_df.append(metric_list)\n",
    "    return ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_interval(ci_df, analytical_perf):\n",
    "    colors = {'A': ['crimson', 'darkred'], 'B': ['darkblue', 'royalblue'], 'P': ['darkgreen', 'limegreen']}\n",
    "    for metric in ci_df:\n",
    "        fig, axis = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        for i, df in enumerate(metric):\n",
    "            x = i // 2\n",
    "            y = i % 2\n",
    "\n",
    "            curr_metric = df['metric'].unique()[0]\n",
    "            curr_node = df['node'].unique()[0]\n",
    "\n",
    "            analitycal_df = analytical_perf[curr_metric][curr_node]\n",
    "            \n",
    "            # error bar plot with mean point\n",
    "            up = df['upper_ci'] - df['mean']\n",
    "            low = df['mean'] - df['lower_ci']\n",
    "            df['lambda'] = df['lambda'].astype(float)\n",
    "\n",
    "            axis[x, y].errorbar(df['lambda'], df['mean'], yerr=[low, up], fmt='o', capsize=5, label='Confidence interval 95%')\n",
    "\n",
    "            # lineplot of the mean points\n",
    "            axis[x, y].plot(df['lambda'], df['mean'], linestyle='-', marker='o', color='b', label='Avg')\n",
    "            axis[x, y].plot(analitycal_df['lambda'], analitycal_df['value'], linestyle='-', marker='o', color='r', label='Avg')\n",
    "\n",
    "            axis[x, y].set_xlabel('Lambda')\n",
    "            axis[x, y].set_ylabel(f\"Avg {curr_metric} in {curr_node}\")\n",
    "            axis[x, y].set_title(f\"Confidence interval for {curr_node} average {curr_metric}\")\n",
    "            axis[x, y].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if 'throughput' in df['metric'].values:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            for df in metric:\n",
    "                if df['node'].unique()[0] == 'SYSTEM':\n",
    "                    continue\n",
    "                base_color = colors[df['node'].unique()[0]][0]\n",
    "                darker_color = colors[df['node'].unique()[0]][1]\n",
    "                ax.errorbar(df['lambda'], df['mean'], \n",
    "                            yerr=[df['mean'] - df['lower_ci'], df['upper_ci'] - df['mean']], \n",
    "                            fmt='o', capsize=5, color = darker_color)\n",
    "\n",
    "                ax.plot(df['lambda'], df['mean'], linestyle='-', marker='o', label=f'Throughput of {df[\"node\"].unique()[0]}', color = base_color)\n",
    "\n",
    "            # Aggiunta delle etichette e del titolo\n",
    "            ax.set_xlabel('Lambda')\n",
    "            ax.set_ylabel('Avg')\n",
    "            ax.set_title('Throughput comparison')\n",
    "            ax.legend()\n",
    "            # Mostra il grafico\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation vs. Analitycal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analitycal_preprocessing(obj):\n",
    "    metric_node_analitycal_dict = {}\n",
    "    servers = ['[A]', '[B]', '[P]', '[S]']\n",
    "    metrics = {'E[T]': 'response_time', 'E[N]': 'population', 'X': 'throughput', 'rho': 'utilization'} \n",
    "\n",
    "    local_perf = pd.read_csv(os.path.join('output', f'{obj}_analitycal_local_performances.csv'))\n",
    "    global_perf = pd.read_csv(os.path.join('output', f'{obj}_analitycal_global_performances.csv'))\n",
    "    rho_perf = pd.read_csv(os.path.join('output', f'{obj}_analitycal_rho.csv'))\n",
    "    analitycal_perf = pd.merge(local_perf, global_perf, on=['gamma'], how='inner', validate='one_to_one')\n",
    "    analitycal_perf = pd.merge(analitycal_perf, rho_perf, on=['gamma'], how='inner', validate='one_to_one')\n",
    "    \n",
    "    for m in metrics.keys():\n",
    "        metric_columns = [col for col in analitycal_perf.columns if col.startswith(m)]\n",
    "        metric_df = analitycal_perf[metric_columns].copy()\n",
    "        metric_df['metric'] = metrics[m]\n",
    "        node_dict = {}\n",
    "        for s in servers:\n",
    "            s_column = [col for col in metric_df.columns if col.endswith(s)]\n",
    "            if not s_column:\n",
    "                continue\n",
    "            s_df = metric_df[s_column].copy()\n",
    "\n",
    "            s_df = s_df.rename(columns={s_column[0]: 'value'})\n",
    "            node = s.split('[')[1].split(']')[0]\n",
    "            node = 'SYSTEM' if node == 'S' else node\n",
    "            s_df['node'] = node\n",
    "            s_df['lambda'] = analitycal_perf['gamma'].copy().astype(float)\n",
    "            s_df['metric'] = metric_df['metric'].unique()[0]\n",
    "            node_dict[node] = s_df\n",
    "        metric_node_analitycal_dict[metrics[m]] = node_dict\n",
    "\n",
    "    return metric_node_analitycal_dict\n",
    "\n",
    "\n",
    "def average_stats(stats_df, analytical_perf):\n",
    "    avg_metric_list = []\n",
    "    for metric in stats_df:\n",
    "        average_list_node = []\n",
    "        for node in metric:\n",
    "            node.drop(columns=['iteration'], inplace=True)\n",
    "            node = node.groupby(['lambda', 'metric', 'node'])['value'].mean().reset_index()\n",
    "            node['lambda'] = node['lambda'].astype(float)\n",
    "            merging_df = analytical_perf[node['metric'].unique()[0]][node['node'].unique()[0]]\n",
    "            \n",
    "            node = pd.merge(node, merging_df, on=['lambda', 'metric', 'node'], how='inner', validate='one_to_one')\n",
    "            node['diff'] = abs(node['value_x'] - node['value_y'])\n",
    "            node.rename(columns={'value_x': 'simulation_value', 'value_y': 'analitycal_value'}, inplace=True)\n",
    "            average_list_node.append(node)\n",
    "        avg_metric_list.append(average_list_node)\n",
    "    return avg_metric_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1\n",
    "\n",
    "Implementation of a model to execute the web app workflow to measure response time (R), population number (N) and throughput (X) quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Merge keys are not unique in right dataset; not a many-to-one merge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m critical_value \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mppf(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.05\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m metrics_nodes_avg_df_list \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m analytical_perf \u001b[38;5;241m=\u001b[39m analitycal_preprocessing(obj)\n\u001b[0;32m      8\u001b[0m boxplot(metrics_nodes_avg_df_list)\n",
      "Cell \u001b[1;32mIn[153], line 54\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(path, obj)\u001b[0m\n\u001b[0;32m     52\u001b[0m     metrics_nodes_avg_df_list\u001b[38;5;241m.\u001b[39mappend(node_avg_df_list)\n\u001b[0;32m     53\u001b[0m metric_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutilization\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthroughput\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m---> 54\u001b[0m derived_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mderived_metrics_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m derived_metrics:\n\u001b[0;32m     56\u001b[0m     metrics_nodes_avg_df_list\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "Cell \u001b[1;32mIn[152], line 26\u001b[0m, in \u001b[0;36mderived_metrics_calc\u001b[1;34m(num_denum_df, obj, num_denum_dict)\u001b[0m\n\u001b[0;32m     23\u001b[0m num_df \u001b[38;5;241m=\u001b[39m num_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m denum_df \u001b[38;5;241m=\u001b[39m denum_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m metric_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenum_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlambda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmany_to_one\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m metric_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_x\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m metric_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_y\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\savel\\Documents\\Universita\\PMCSN\\Progetto\\webapp-workflow-pa\\.conda\\sim-env-dev\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\savel\\Documents\\Universita\\PMCSN\\Progetto\\webapp-workflow-pa\\.conda\\sim-env-dev\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:813\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_validate_kwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\savel\\Documents\\Universita\\PMCSN\\Progetto\\webapp-workflow-pa\\.conda\\sim-env-dev\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1669\u001b[0m, in \u001b[0;36m_MergeOperation._validate_validate_kwd\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmany_to_one\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_unique:\n\u001b[1;32m-> 1669\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1670\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in right dataset; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1671\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot a many-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1672\u001b[0m         )\n\u001b[0;32m   1674\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmany_to_many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm:m\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1675\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mMergeError\u001b[0m: Merge keys are not unique in right dataset; not a many-to-one merge"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\"..\", \"..\", \"src\", \"caballo\", \"domestico\", \"wwsimulator\", \"statistics\", \"objective_1\", 'BatchMeansSimulation')\n",
    "critical_value = scipy.stats.norm.ppf(1-.05/2)\n",
    "obj = 'obj1'\n",
    "\n",
    "metrics_nodes_avg_df_list = preprocessing(path, obj)\n",
    "analytical_perf = analitycal_preprocessing(obj)\n",
    "\n",
    "boxplot(metrics_nodes_avg_df_list)\n",
    "ci_df = confidence_interval(metrics_nodes_avg_df_list, critical_value, obj)\n",
    "plot_confidence_interval(ci_df, analytical_perf)\n",
    "\n",
    "avg_metrics_list = average_stats(metrics_nodes_avg_df_list, analytical_perf)\n",
    "for metric in avg_metrics_list:\n",
    "    for node in metric:\n",
    "        df = node.reset_index()\n",
    "        df.drop(columns=['index'], inplace=True)\n",
    "        display(tabulate(df, headers='keys', tablefmt='html', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1 - Transient state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\", \"..\", \"src\", \"caballo\", \"domestico\", \"wwsimulator\", \"statistics\", \"objective_1\", 'ReplicatedSimulation')\n",
    "critical_value = scipy.stats.norm.ppf(1-.05/2)\n",
    "obj = 'obj1'\n",
    "\n",
    "metrics_nodes_avg_df_list = preprocessing(path, obj)\n",
    "analytical_perf = analitycal_preprocessing(obj)\n",
    "\n",
    "boxplot(metrics_nodes_avg_df_list)\n",
    "ci_df = confidence_interval(metrics_nodes_avg_df_list, critical_value, obj)\n",
    "plot_confidence_interval(ci_df, analytical_perf)\n",
    "\n",
    "avg_metrics_list = average_stats(metrics_nodes_avg_df_list, analytical_perf)\n",
    "for metric in avg_metrics_list:\n",
    "    for node in metric:\n",
    "        df = node.reset_index()\n",
    "        df.drop(columns=['index'], inplace=True)\n",
    "        display(tabulate(df, headers='keys', tablefmt='html', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2\n",
    "\n",
    "The model adds the functionality of two-factor authentication, observe the three metrics variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\", \"..\", \"src\", \"caballo\", \"domestico\", \"wwsimulator\", \"statistics\", \"objective_2\", 'BatchMeansSimulation')\n",
    "critical_value = scipy.stats.norm.ppf(1-.05/2)\n",
    "obj = 'obj2'\n",
    "\n",
    "metrics_nodes_avg_df_list = preprocessing(path, obj)\n",
    "analytical_perf = analitycal_preprocessing(obj)\n",
    "\n",
    "boxplot(metrics_nodes_avg_df_list)\n",
    "\n",
    "ci_df = confidence_interval(metrics_nodes_avg_df_list, critical_value, obj)\n",
    "plot_confidence_interval(ci_df, analytical_perf)\n",
    "\n",
    "avg_metrics_list = average_stats(metrics_nodes_avg_df_list, analytical_perf)\n",
    "for metric in avg_metrics_list:\n",
    "    for node in metric:\n",
    "        df = node.reset_index()\n",
    "        df.drop(columns=['index'], inplace=True)\n",
    "        display(tabulate(df, headers='keys', tablefmt='html', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3\n",
    "\n",
    "Measure the metrics with an increased workload from 4200 req/h to 5000 req/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\", \"..\", \"src\", \"caballo\", \"domestico\", \"wwsimulator\", \"statistics\", \"objective_3\", 'BatchMeansSimulation')\n",
    "critical_value = scipy.stats.norm.ppf(1-.05/2)\n",
    "obj = 'obj3'\n",
    "\n",
    "metrics_nodes_avg_df_list = preprocessing(path, obj)\n",
    "analytical_perf = analitycal_preprocessing(obj)\n",
    "\n",
    "boxplot(metrics_nodes_avg_df_list)\n",
    "\n",
    "ci_df = confidence_interval(metrics_nodes_avg_df_list, critical_value, obj)\n",
    "plot_confidence_interval(ci_df, analytical_perf)\n",
    "\n",
    "avg_metrics_list = average_stats(metrics_nodes_avg_df_list, analytical_perf)\n",
    "for metric in avg_metrics_list:\n",
    "    for node in metric:\n",
    "        df = node.reset_index()\n",
    "        df.drop(columns=['index'], inplace=True)\n",
    "        display(tabulate(df, headers='keys', tablefmt='html', showindex=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim-env-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
